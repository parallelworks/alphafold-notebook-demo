{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular Dynamics Lite workflow\n",
    "This notebook implements a simple molecular dynamics (MD) workflow to demonstrate [Parsl Python parallel scripting](https://parsl-project.org/) in a Jupyter notebook. This workflow first runs MD simulations in parallel on remote resources and then renders the frames of an animation visualizing the simulation according to the schematic below. This workflow requires a Conda environment with Parsl (see last cell for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define workflow inputs\n",
    "This PW workflow can be either launched from its form in the `Compute` tab or it can be run directly in this notebook.  If running directly from the notebook, the user needs to go through the extra step of defining the inputs of the workfow in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config line 0: einsteindefault http://localhost:64107\n",
      "pwargs: Namespace(f='/gs/gsfs0/users/gstefan/.local/share/jupyter/runtime/kernel-3529c7b1-c481-47ed-a1b8-10e26cf59d97.json')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparslpw\u001b[39;00m\n",
      "File \u001b[0;32m/pw/workflows/alphafold_notebook/utils/parslpw.py:134\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# For embedding Design Explorer results in notebook\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#from IPython.display import display, HTML\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Start logging\u001b[39;00m\n\u001b[1;32m    123\u001b[0m parsl\u001b[38;5;241m.\u001b[39mset_stream_logger(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m    125\u001b[0m pwconfig \u001b[38;5;241m=\u001b[39m Config(\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m#run_dir='./local_parsl_logs',          # Defaults to ./runinfo\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     executors\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    128\u001b[0m         HighThroughputExecutor(\n\u001b[1;32m    129\u001b[0m             label\u001b[38;5;241m=\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool\u001b[39m\u001b[38;5;124m'\u001b[39m],         \u001b[38;5;66;03m# This value names the pools in the log dirs\u001b[39;00m\n\u001b[1;32m    130\u001b[0m             address\u001b[38;5;241m=\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmLoginNode\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# IP or hostname visible to the workers/remote resource for connecting back to interchange!!\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             worker_debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,             \u001b[38;5;66;03m# Default False for shorter logs\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    133\u001b[0m             mem_per_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m),         \u001b[38;5;66;03m# Used to find number of slots per node\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m             cores_per_worker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpwprops\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjobsPerNode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,       \u001b[38;5;66;03m# DOES NOT correspond to --cpus-per-task 1 per Parsl docs.  Rather BYPASSES SLURM opts and is process_pool.py -c cores_per_worker, but IS NOT CORES ON SLURM - sets number of workers\u001b[39;00m\n\u001b[1;32m    135\u001b[0m             working_dir \u001b[38;5;241m=\u001b[39m pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkdir\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pw\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    136\u001b[0m             worker_logdir_root \u001b[38;5;241m=\u001b[39m pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkdir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pworks/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    137\u001b[0m             provider \u001b[38;5;241m=\u001b[39m SlurmProvider(\n\u001b[1;32m    138\u001b[0m                 partition \u001b[38;5;241m=\u001b[39m pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmPartition\u001b[39m\u001b[38;5;124m'\u001b[39m],          \u001b[38;5;66;03m# Cluster specific! Needs to match GPU availability, and RAM per CPU limits specified for partion.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m                 channel\u001b[38;5;241m=\u001b[39mSSHChannel(\n\u001b[1;32m    140\u001b[0m                     hostname\u001b[38;5;241m=\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmLoginNode\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;66;03m#slurm_user+'.hpc.einsteinmed.edu',\u001b[39;00m\n\u001b[1;32m    141\u001b[0m                     username\u001b[38;5;241m=\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmUsername\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    142\u001b[0m                     key_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gs/gsfs0/users/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmUsername\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/.ssh/pw_id_rsa\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    143\u001b[0m                     script_dir\u001b[38;5;241m=\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkdir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pworks/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    144\u001b[0m                 ),\n\u001b[1;32m    145\u001b[0m                 worker_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource /gs/gsfs0/users/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmUsername\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pworks/bootstrap.sh; source /gs/gsfs0/hpc01/rhel8/apps/conda3/etc/profile.d/conda.sh; conda activate /gs/gsfs0/users/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmUsername\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pw/parsl-1.2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    146\u001b[0m                 init_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    147\u001b[0m                 mem_per_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    148\u001b[0m                 nodes_per_block \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmNodesPerBlock\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    149\u001b[0m                 cores_per_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmPPN\u001b[39m\u001b[38;5;124m'\u001b[39m]),   \u001b[38;5;66;03m# Corresponds to --cpus-per-task\u001b[39;00m\n\u001b[1;32m    150\u001b[0m                 min_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    151\u001b[0m                 max_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    152\u001b[0m                 parallelism \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,           \u001b[38;5;66;03m# Was 0.80, 1 is \"use everything you can NOW\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m                 exclusive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,         \u001b[38;5;66;03m# Default is T, hard to get workers on shared cluster\u001b[39;00m\n\u001b[1;32m    154\u001b[0m                 walltime\u001b[38;5;241m=\u001b[39mpwprops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslurmWalltime\u001b[39m\u001b[38;5;124m'\u001b[39m],       \u001b[38;5;66;03m# Will limit job to this run time, 10 min default Parsl\u001b[39;00m\n\u001b[1;32m    155\u001b[0m                 launcher\u001b[38;5;241m=\u001b[39mSrunLauncher()    \u001b[38;5;66;03m# defaults to SingleNodeLauncher() which seems to work\u001b[39;00m\n\u001b[1;32m    156\u001b[0m             ),\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;66;03m#storage_access=[RSyncStaging('gstefan@gstefan.hpc.einsteinmed.edu')]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m             storage_access\u001b[38;5;241m=\u001b[39m[NoOpFileStaging()]\n\u001b[1;32m    159\u001b[0m         )\n\u001b[1;32m    160\u001b[0m     ]\n\u001b[1;32m    161\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "import utils.parslpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "print('Define workflow inputs...')\n",
    "\n",
    "# Start assuming workflow is launched from the form.\n",
    "run_in_notebook=False\n",
    "\n",
    "if (exists(\"./params.run\")):\n",
    "    print(\"Running from a PW form.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Running from a notebook.\")\n",
    "    \n",
    "    # Set flag for later\n",
    "    run_in_notebook=True\n",
    "    \n",
    "    # Manually set workflow inputs here (same as the\n",
    "    # default values in workflow launch form)\n",
    "    # The ranges of EACH dimension in the parameter\n",
    "    # sweep are defined by the format:\n",
    "    #\n",
    "    # NAME;input;MIN:MAX:STEP\n",
    "    #\n",
    "    #=========================================\n",
    "    # npart = number of particles\n",
    "    # steps = time steps in simulation\n",
    "    # mass = mass of partiles\n",
    "    # trsnaps = number of frames (\"snapshots\") of simulation for animation\n",
    "    #=========================================\n",
    "    params=\"npart;input;25:50:25|steps;input;3000:6000:3000|mass;input;0.01:0.02:0.01|trsnaps;input;5:10:5|\"\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    # Write to params.run\n",
    "    with open(\"params.run\",\"w\") as f:\n",
    "        n_char_written = f.write(params+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Parsl\n",
    "The molecular dynamics software itself is a lightweight, precompiled executable written in C. The executable is distributed with this workflow in `./models/mdlite`, and along with input files, it is staged to the remote resources and does not need to be preinstalled.\n",
    "\n",
    "The core visualization tool used here is a precompiled binary of [c-ray](https://github.com/vkoskiv/c-ray) distributed with this workflow in `./models/c-ray`. The executable is staged to remote resources and does not need to be preinstalled.\n",
    "\n",
    "In addition to a Miniconda environment containing Parsl, the only other dependency of this workflow is ImageMagick's `convert` tool for image format conversion (`.ppm` to `.png`) and building animated `.gif` files from `.png` frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parsl essentials\n",
    "import parsl\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.data_provider.files import File\n",
    "from parsl.config import Config\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import SlurmProvider\n",
    "from parsl.launchers import SrunLauncher\n",
    "from parsl.channels import SSHChannel,SSHInteractiveLoginChannel\n",
    "#from parsl.data_provider.rsync import RSyncStaging\n",
    "from parsl.data_provider.file_noop import NoOpFileStaging\n",
    "import logging # Needed for parsl.set_file_logger\n",
    "\n",
    "# PW essentials\n",
    "from utils.path import Path\n",
    "# GET COMMAND LINE ARGS FROM PW FORM\n",
    "import argparse\n",
    "parser=argparse.ArgumentParser()\n",
    "parsed, unknown = parser.parse_known_args()\n",
    "for arg in unknown:\n",
    "    if arg.startswith((\"-\", \"--\")):\n",
    "        parser.add_argument(arg)\n",
    "pwargs=parser.parse_args()\n",
    "print(\"pwargs:\",pwargs)\n",
    "\n",
    "# For embedding Design Explorer results in notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Checking inputs from the WORKFLOW FORM\n",
    "if (not run_in_notebook):\n",
    "    print(pwargs)\n",
    "\n",
    "# Start logging\n",
    "parsl.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "#==============================\n",
    "# Key setup parameters\n",
    "#==============================\n",
    "slurm_user = 'gstefan'\n",
    "home_dir = '/gs/gsfs0/users/'+slurm_user+'/'\n",
    "workflow_dir = home_dir+'pw/workflows/mdlite/'\n",
    "\n",
    "# Other customizations, not critical.\n",
    "work_dir = 'parsl_work'\n",
    "log_dir = 'parsl_log'\n",
    "script_dir = 'parsl_script'\n",
    "\n",
    "print(\"Configuring Parsl...\")\n",
    "config = Config(\n",
    "    run_dir='./local_parsl_logs',          # Defaults to ./runinfo\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            label=slurm_user+'_slurm',         # This value names the pools in the log dirs\n",
    "            address='einsteinmed-submit.parallel.works',  # IP or hostname visible to the workers/remote resource for connecting back to interchange!!\n",
    "            worker_debug=False,             # Default False for shorter logs\n",
    "            max_workers=int(100),\n",
    "            mem_per_worker=int(1),         # Used to find number of slots per node\n",
    "            cores_per_worker=int(1),       # DOES NOT correspond to --cpus-per-task 1 per Parsl docs.  Rather BYPASSES SLURM opts and is process_pool.py -c cores_per_worker, but IS NOT CORES ON SLURM - sets number of workers                                                       \n",
    "            working_dir = home_dir+work_dir,\n",
    "            worker_logdir_root = home_dir+log_dir,\n",
    "            provider = SlurmProvider(\n",
    "                partition = 'test',          # Cluster specific! Needs to match GPU availability, and RAM per CPU limits specified for partion.\n",
    "                channel=SSHChannel(\n",
    "                    hostname='einsteinmed-submit.parallel.works', #slurm_user+'.hpc.einsteinmed.edu',\n",
    "                    username=slurm_user,\n",
    "                    key_filename='/gs/gsfs0/users/'+slurm_user+'/.ssh/pw_id_rsa',\n",
    "                    script_dir=home_dir+script_dir\n",
    "                ),\n",
    "                worker_init='source /gs/gsfs0/users/'+slurm_user+'/pworks/bootstrap.sh; source /gs/gsfs0/hpc01/rhel8/apps/conda3/etc/profile.d/conda.sh; conda activate /gs/gsfs0/users/'+slurm_user+'/pw/parsl-1.2',\n",
    "                init_blocks=1,\n",
    "                mem_per_node = int(2),\n",
    "                nodes_per_block = int(1),\n",
    "                cores_per_node = int(1),   # Corresponds to --cpus-per-task                                                                                                                         \n",
    "                min_blocks = int(1),\n",
    "                max_blocks = int(10),\n",
    "                parallelism = 1,           # Was 0.80, 1 is \"use everything you can NOW\"                                                                                                            \n",
    "                exclusive = False,         # Default is T, hard to get workers on shared cluster                                                                                                    \n",
    "                walltime='00:10:00',       # Will limit job to this run time, 10 min default Parsl                                                                                                  \n",
    "                launcher=SrunLauncher()    # defaults to SingleNodeLauncher() which seems to work\n",
    "            ),\n",
    "            #storage_access=[RSyncStaging('gstefan@gstefan.hpc.einsteinmed.edu')]\n",
    "            storage_access=[NoOpFileStaging()]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "parsl.load(config)\n",
    "print(\"Parsl configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Parsl workflow apps\n",
    "These apps are decorated with Parsl's `@bash_app` and as such are executed in parallel on the compute resources that are defined in the PW configuration loaded above.  Functions that are **not** decorated are not executed in parallel on remote resources. The files that need to be staged to remote resources will be marked with Parsl's `File()` (or its PW extension, `Path()`) in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining Parsl workflow apps...\")\n",
    "\n",
    "#===================================\n",
    "# Molecular dynamics simulation app\n",
    "#===================================\n",
    "@bash_app\n",
    "def md_run(stdout='md.run.stdout', stderr='md.run.stderr', inputs=[], outputs=[]):\n",
    "    return '''\n",
    "    outdir=%s\n",
    "    mkdir -p $outdir\n",
    "    cd $outdir\n",
    "    %s/runMD.sh \"%s\" metric.out trj.out\n",
    "    ''' % (outputs[0],inputs[1],inputs[0])\n",
    "\n",
    "@bash_app\n",
    "def md_run_2(stdout='md.run.stdout', stderr='md.run.stderr', inputs=[], outputs=[]):\n",
    "    return '''\n",
    "    %s/runMD.sh \"%s\" metric.out trj.out\n",
    "    outdir=%s\n",
    "    mkdir -p $outdir\n",
    "    mv trj.out $outdir/\n",
    "    mv metric.out $outdir/\n",
    "    ''' % (inputs[1],inputs[0],outputs[0])\n",
    "\n",
    "\n",
    "#===================================\n",
    "# App to render frames for animation\n",
    "#===================================\n",
    "# All frames for a given simulation\n",
    "# are rendered together.\n",
    "\n",
    "# This app takes a very simple \n",
    "# approach to zero padding by adding \n",
    "# integers to 1000.\n",
    "@bash_app\n",
    "def md_vis_3(stdout='md.vis.stdout', stderr='md.vis.stderr', inputs=[], outputs=[]):\n",
    "    return '''\n",
    "    indir=%s\n",
    "    outdir=%s\n",
    "    mkdir -p $outdir\n",
    "    for (( ff=0; ff<%s; ff++ ))\n",
    "    do\n",
    "        frame_num_padded=$((1000+$ff))\n",
    "        %s/renderframe_shared_fs $indir/trj.out $outdir/f_$frame_num_padded.ppm $ff\n",
    "    done\n",
    "    ''' % (inputs[2],outputs[0],inputs[0],inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Workflow\n",
    "This cell executes the workflow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Running workflow...\")\n",
    "\n",
    "#============================================================================\n",
    "# SETUP PARAMETER SWEEP\n",
    "#============================================================================\n",
    "# Generate a case list from params.run (the ranges to parameters to sweep)\n",
    "os.system(\"python ./models/mexdex/prepinputs.py params.run cases.list\")\n",
    "\n",
    "# Each line in cases.list is a unique combination of the parameters to sweep.\n",
    "with open(\"cases.list\",\"r\") as f:\n",
    "    cases_list = f.readlines()\n",
    "\n",
    "#============================================================================\n",
    "# SIMULATE\n",
    "#============================================================================\n",
    "# For each line in cases.list, run and visualize a molecular dynamics simulation\n",
    "# These empty lists will store the futures of Parsl-parallelized apps.\n",
    "# Use Path for staging because multiple files in ./models/mdlite are needed\n",
    "# and mutliple files in ./results/case_*/md are sent back to the platform.\n",
    "md_run_fut = []\n",
    "for ii, case in enumerate(cases_list):        \n",
    "    # Run simulation\n",
    "    md_run_fut.append(md_run(\n",
    "        inputs=[case,\n",
    "            File(workflow_dir+\"models/mdlite\")],\n",
    "        outputs=[File(workflow_dir+\"results/case_\"+str(ii)+\"/md\")]))\n",
    "    \n",
    "# Call results for all app futures to require\n",
    "# execution to wait for all simulations to complete.\n",
    "for run in md_run_fut:\n",
    "    run.result()\n",
    "\n",
    "#============================================================================\n",
    "# VISUALIZE\n",
    "#============================================================================\n",
    "md_vis_fut = []\n",
    "for ii, case in enumerate(cases_list):\n",
    "    # Get number of frames to render for this case\n",
    "    nframe = int(case.split(',')[4])\n",
    "    \n",
    "    #=========================================================\n",
    "    # Render all frames for each case in one app.  This approach\n",
    "    # reduces the number of SSH connections (e.g. rsync instances) \n",
    "    # compared to an app that only renders one frame at a time.\n",
    "    md_vis_fut.append(md_vis_3(\n",
    "        inputs=[nframe,\n",
    "                File(workflow_dir+\"models/c-ray\"),\n",
    "                File(workflow_dir+\"results/case_\"+str(ii)+\"/md\")],\n",
    "        outputs=[File(workflow_dir+\"results/case_\"+str(ii)+\"/vis\")]))\n",
    "\n",
    "for vis in md_vis_fut:\n",
    "    vis.result()\n",
    "\n",
    "print(\"Tasks completed - compiling frames into animations...\")\n",
    "\n",
    "# Compile frames into movies locally\n",
    "for ii, case in enumerate(cases_list):\n",
    "    os.system(\"cd ./results/case_\"+str(ii)+\"/vis; convert -delay 10 *.ppm mdlite.gif\")\n",
    "\n",
    "# Compile movies into Design Explorer results locally\n",
    "os.system(\"./models/mexdex/postprocess.sh mdlite_dex.csv mdlite_dex.html ./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View results\n",
    "This step is only necessary when running directly in a notebook. The outputs of this workflow are stored in the `results` folder and they can be interactively visualized with the Design Explorer by clicking on `mdlite_dex.html` which uses `mdlite_dex.csv` and the data in the `results` folder. The Design Explorer visualization is automatically embedded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Modify width and height to display as wanted\n",
    "from IPython.display import IFrame\n",
    "def designExplorer(url,height=600):\n",
    "    return IFrame(url, width='100%', height=height)\n",
    "\n",
    "\n",
    "# Makre sure path to datafile=/pw/workflows/mdlite/mdlite_dex.csv is correct\n",
    "designExplorer(\n",
    "    '/preview/DesignExplorer/index.html?datafile=/pw/workflows/mdlite/mdlite_dex.csv&colorby=kinetic',\n",
    "    height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================\n",
    "# Alphafold EXAMPLE output: 3D visualization in notebook\n",
    "#=============================================================\n",
    "# The images displayed below do NOT come from this simulation\n",
    "# but they are provided here as an example of a more complex\n",
    "# workflow. In particular, if an image is selected, the 3D\n",
    "# radio button can be used to interactively rotate molecules.\n",
    "from IPython.display import IFrame\n",
    "def designExplorer(url,height=600):\n",
    "    return IFrame(url, width='100%', height=height)\n",
    "\n",
    "designExplorer(\n",
    "    '/preview/DesignExplorer/index.html?datafile=/pw/workflows/mdlite/models/sample_af/sample.csv&colorby=plddt',\n",
    "    height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Use notebook to interact directly with simulation results\n",
    "Jupyter notebooks are great because cells can be re-run in isolation as ideas are fine-tuned.  The cell below allows for plotting a new result directly from the simulation outputs; there is no need to re-run the simulation if the plot needs to be modified as the user explores the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data are in the results/case_* folders.\n",
    "list_of_cases = glob.glob(\"results/case_*\")\n",
    "\n",
    "# Initialize lists to store data for plotting\n",
    "cases = []\n",
    "all_cases_time_val = []\n",
    "all_cases_rt_mean_sq_std = []\n",
    "all_cases_rt_mean_sq_mean = []\n",
    "\n",
    "# Loop through each case\n",
    "for case in list_of_cases:\n",
    "\n",
    "    # Get info about this case\n",
    "    path = case + \"/md/trj.out\"\n",
    "    case_name = case[case.index('case'):]\n",
    "    cases.append(case_name)\n",
    "    \n",
    "    # Load data for this case\n",
    "    data = pd.read_csv(path, sep=\" \")\n",
    "    data.columns=['time', 'var', 'x_pos', 'y_pos', 'z_pos', 'ig0', 'ig1', 'ig2', 'ig3', 'ig4', 'ig5']\n",
    "    t_val = data['time'].unique()\n",
    "    all_cases_time_val.append(t_val)\n",
    "    \n",
    "    # Create and initialize lists of root mean square for std and mean\n",
    "    one_case_rt_mean_sq_std = []\n",
    "    one_case_rt_mean_sq_mean = []\n",
    "\n",
    "    # Loop through each instance in time and compute statistics\n",
    "    for t in t_val:\n",
    "\n",
    "        each_time = data.loc[data['time'] == t, 'x_pos':'z_pos']\n",
    "        all_pos_std = each_time.std()\n",
    "        all_pos_mean = each_time.mean()\n",
    "        \n",
    "        # Calculate root mean square of std and mean (vector magnitude)\n",
    "        # Fix decimal points to 6\n",
    "        rt_mean_sq_std = math.sqrt((all_pos_std['x_pos'])**2 + (all_pos_std['y_pos'])**2 + (all_pos_std['z_pos'])**2)\n",
    "        one_case_rt_mean_sq_std.append(round(rt_mean_sq_std,6))\n",
    "        rt_mean_sq_mean = math.sqrt((all_pos_mean['x_pos'])**2 + (all_pos_mean['y_pos'])**2 + (all_pos_mean['z_pos'])**2)\n",
    "        one_case_rt_mean_sq_mean.append(round(rt_mean_sq_mean,6))\n",
    "        \n",
    "    # After getting all root mean square for std and mean of all time,\n",
    "    # put it in the list for all cases.\n",
    "    all_cases_rt_mean_sq_std.append(one_case_rt_mean_sq_std)\n",
    "    all_cases_rt_mean_sq_mean.append(one_case_rt_mean_sq_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot side by side root mean square std vs. time \n",
    "# and root mean square mean vs. time\n",
    "fig, (ax0, ax1) = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "# Go through each cases to plot\n",
    "# If desired to see some case not all,\n",
    "# could change range(len(cases)) to range(<some number less than len(cases)>)\n",
    "for c in range(len(cases)):\n",
    "    # Plot root mean square std vs. time with solid line\n",
    "    # and dots for each value (x,y) on the graph\n",
    "    # x-axis is time, y-axis is root mean square std\n",
    "    ax0.plot(all_cases_time_val[c],all_cases_rt_mean_sq_std[c],'-o')\n",
    "    ax0.set_xlabel('Time(s)', fontsize=20)\n",
    "    ax0.set_ylabel('RMS variance of positions', fontsize=15)\n",
    "\n",
    "    # Plot root mean square mean vs. time with solid line\n",
    "    # and squares for each value (x,y) on the graph\n",
    "    # x-axis is time, y-axis is root mean square mean\n",
    "    ax1.plot(all_cases_time_val[c],all_cases_rt_mean_sq_mean[c],'-s')\n",
    "    ax1.set_xlabel('Time(s)', fontsize=20)\n",
    "    ax1.set_ylabel('Magnitude of centroid position', fontsize=15)\n",
    "    \n",
    "# Add legend to show name of each case\n",
    "ax0.legend(cases)\n",
    "ax1.legend(cases)\n",
    "\n",
    "# Add title for each plot\n",
    "ax0.set_title(\"Spread of particle swarm\",\n",
    "              fontsize=25)\n",
    "ax1.set_title(\"Centroid of particle swarm\",\n",
    "              fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Clean up\n",
    "This step is only necessary when running directly in a notebook. These intermediate and log files are removed to keep the workflow file structure clean if this workflow is pushed into the PW Market Place.  Please feel free to comment out these lines in order to inspect intermediate files as needed. The first two, `params.run` and `cases.list` are explicitly created by the workflow in Steps 1 and 4, respectively.  The other files are generated automatically for logging, keeping track of workers, or starting up workers. **Note that even the results are deleted!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_in_notebook):\n",
    "    # Delete intermediate files/logs that are NOT core code or results\n",
    "    !rm -f params.run\n",
    "    !rm -f cases.list\n",
    "    !rm -rf runinfo\n",
    "    !rm -rf __pycache__\n",
    "    !rm -rf parsl-task.*\n",
    "    !rm -rf *.pid\n",
    "    !rm -rf *.started\n",
    "    !rm -rf *.ended\n",
    "    !rm -rf *.cancelled\n",
    "    !rm -rf *.cogout\n",
    "    !rm -rf lastid*\n",
    "    !rm -rf launchcmd.*\n",
    "    !rm -rf parsl-htex-worker.sh\n",
    "    # Retain pw.conf if re-running this notebook on the \n",
    "    # same resource and there is no resource Off/On cycling.\n",
    "    # (See README.md for more information.)\n",
    "    !rm -rf pw.conf\n",
    "    # Delete outputs\n",
    "    #!rm -rf ./results\n",
    "    #!rm -f mdlite_dex.*\n",
    "    \n",
    "    # shut down the parsl executor\n",
    "    parsl.dfk().cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conda environment setup instructions\n",
    "This workflow can run with standard Parsl (currently `parsl-1.2`) or with Parallel Works modified Parsl (`parsl-pw`). Standard Parsl is preferred.\n",
    "\n",
    "If running with the parsl-pw local environment, it is built like this:\n",
    "```bash\n",
    "# Full path to home dir so avail on submit and worker\n",
    "conda create -y -p /gs/gsfs0/users/gstefan/pw/parsl-pw -c anaconda jinja2 requests ipykernel\n",
    "\n",
    "conda activate /gs/gsfs0/users/gstefan/pw/parsl-pw\n",
    "\n",
    "# Needed for Parsl\n",
    "conda install -y ipython_genutils\n",
    "\n",
    "# Need to pip install parsl-pw\n",
    "cd /swift-pw-bin/parslab/build\n",
    "pip install .\n",
    "```\n",
    "\n",
    "If running with standard Parsl, build a current parsl environment for parsl-1.2:\n",
    "```bash\n",
    "# Full path to home dir so avail on submit and worker\n",
    "conda create -y -p /gs/gsfs0/users/gstefan/pw/parsl-1.2 -c anaconda -c conda-forge jinja2 requests ipykernel parsl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:parsl-1.2]",
   "language": "python",
   "name": "conda-env-parsl-1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
