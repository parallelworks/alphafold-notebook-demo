{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alphafold workflow\n",
    "This notebook implements an Alphafold workflow to demonstrate [Parsl Python parallel scripting](https://parsl-project.org/) in a Jupyter notebook orchestrating a batch of jobs on a SLURM cluster. The goal is to demonstrate Alphafold running multiple proteins at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define workflow inputs\n",
    "This PW workflow can be either launched from its form in the `Compute` tab or it can be run directly in this notebook.  If running directly from the notebook, the user needs to go through the extra step of defining the inputs of the workfow in the notebook. Currently, this workflow only has one input, which is a list of `.fasta` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting here with automatically loading the \n",
    "# Parsl configuration from a file generated by the\n",
    "# resource.\n",
    "#import utils.parslpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define workflow inputs...\n",
      "Running from a notebook. Use the file specified by the user below.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import argparse\n",
    "\n",
    "print('Define workflow inputs...')\n",
    "\n",
    "# Start assuming workflow is launched from the notebook.\n",
    "run_in_notebook=True\n",
    "\n",
    "if (run_in_notebook):\n",
    "    print(\"Running from a notebook. Use the file specified by the user below.\")\n",
    "    \n",
    "    # Each line in this file is a protein to run with Alphafold.\n",
    "    with open(\"./example/fasta_list.txt\",\"r\") as f:\n",
    "        fasta_list = f.readlines()\n",
    "        \n",
    "else:\n",
    "    print(\"Running from a form.  Use the file specified by the user on the form.\")\n",
    "    run_in_notebook=False\n",
    "    # GET COMMAND LINE ARGS FROM PW FORM\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parsed, unknown = parser.parse_known_args()\n",
    "    for arg in unknown:\n",
    "        if arg.startswith((\"-\", \"--\")):\n",
    "            parser.add_argument(arg)\n",
    "    pwargs=parser.parse_args()\n",
    "    print(\"pwargs:\",pwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Parsl\n",
    "The Alphafold application itself is in a Singularity container.  Instructions for building and testing this container are in `./container/README.md`.\n",
    "\n",
    "The configuration below tells Parsl, the Python parallel scripting library, what kind of compute resources we are using. It is very similar to the `#SBATCH` commands at the top of an `sbatch` script (e.g. `./container/sbatch_example.sh`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/gsfs0/users/gstefan/pw/parsl-1.2/lib/python3.10/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "2022-08-13 04:15:48 parsl.dataflow.dflow:86 [INFO]  Parsl version: 1.2.0\n",
      "2022-08-13 04:15:48 parsl.dataflow.dflow:114 [INFO]  Run id is: 2373a088-5f4e-47bc-b959-19c8e543810e\n",
      "2022-08-13 04:15:49 parsl.dataflow.memoization:164 [INFO]  App caching initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Parsl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 04:15:51 parsl.providers.slurm.slurm:236 [ERROR]  Retcode:1 STDOUT: STDERR:sbatch: error: If munged is up, restart with --num-threads=10\n",
      "sbatch: error: Munge encode failed: Failed to access \"/var/run/munge/munge.socket.2\": No such file or directory\n",
      "sbatch: error: slurm_send_node_msg: g_slurm_auth_create: REQUEST_SUBMIT_BATCH_JOB has authentication error: Invalid authentication credential\n",
      "sbatch: error: Batch job submission failed: Protocol authentication error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission of command to scale_out failed\n",
      "Parsl configuration loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 04:15:55 parsl.executors.status_handling:111 [ERROR]  Setting bad state due to exception\n",
      "Exception: 1. Failed to start block 0: Executor slurm failed due to: Attempts to provision nodes via provider has failed\n",
      "\n",
      "2022-08-13 04:16:00 parsl.executors.status_handling:111 [ERROR]  Setting bad state due to exception\n",
      "Exception: 1. Failed to start block 0: Executor slurm failed due to: Attempts to provision nodes via provider has failed\n",
      "\n",
      "2022-08-13 04:16:05 parsl.executors.status_handling:111 [ERROR]  Setting bad state due to exception\n",
      "Exception: 1. Failed to start block 0: Executor slurm failed due to: Attempts to provision nodes via provider has failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parsl essentials\n",
    "import parsl\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.data_provider.files import File\n",
    "from parsl.config import Config\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import SlurmProvider\n",
    "from parsl.launchers import SrunLauncher\n",
    "from parsl.channels import SSHChannel,SSHInteractiveLoginChannel\n",
    "from parsl.data_provider.file_noop import NoOpFileStaging\n",
    "import logging # Needed for parsl.set_file_logger\n",
    "\n",
    "# For embedding Design Explorer results in notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Checking inputs from the WORKFLOW FORM\n",
    "if (not run_in_notebook):\n",
    "    print(pwargs)\n",
    "\n",
    "# Start logging\n",
    "parsl.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "#==============================\n",
    "# Key setup parameters\n",
    "#==============================\n",
    "slurm_user = 'gstefan'\n",
    "home_dir = '/gs/gsfs0/users/'+slurm_user+'/'\n",
    "workflow_dir = home_dir+'pw/workflows/alphafold-notebook-demo/'\n",
    "\n",
    "# Other customizations, not critical.\n",
    "work_dir = 'parsl_work'\n",
    "log_dir = 'parsl_log'\n",
    "script_dir = 'parsl_script'\n",
    "\n",
    "print(\"Configuring Parsl...\")\n",
    "config = Config(\n",
    "    run_dir='./local_parsl_logs',          # Defaults to ./runinfo\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            label=slurm_user+'_slurm',         # This value names the pools in the log dirs\n",
    "            address='einsteinmed-submit.parallel.works',  # IP or hostname visible to the workers/remote resource for connecting back to interchange!!\n",
    "            worker_debug=False,             # Default False for shorter logs\n",
    "            max_workers=int(100),\n",
    "            mem_per_worker=int(1),         # Used to find number of slots per node\n",
    "            cores_per_worker=int(1),       # DOES NOT correspond to --cpus-per-task 1 per Parsl docs.  Rather BYPASSES SLURM opts and is process_pool.py -c cores_per_worker, but IS NOT CORES ON SLURM - sets number of workers                                                       \n",
    "            working_dir = home_dir+work_dir,\n",
    "            worker_logdir_root = home_dir+log_dir,\n",
    "            provider = SlurmProvider(\n",
    "                partition = 'ht',          # Cluster specific! Needs to match GPU availability, and RAM per CPU limits specified for partion.\n",
    "                channel=SSHChannel(\n",
    "                    hostname='einsteinmed-submit.parallel.works', #slurm_user+'.hpc.einsteinmed.edu',\n",
    "                    username=slurm_user,\n",
    "                    key_filename='/gs/gsfs0/users/'+slurm_user+'/.ssh/pw_id_rsa',\n",
    "                    script_dir=home_dir+script_dir\n",
    "                ),\n",
    "                worker_init='source /gs/gsfs0/users/'+slurm_user+'/pworks/bootstrap.sh; source /gs/gsfs0/hpc01/rhel8/apps/conda3/etc/profile.d/conda.sh; conda activate /gs/gsfs0/users/'+slurm_user+'/pw/parsl-1.2',\n",
    "                init_blocks=1,\n",
    "                mem_per_node = int(120),\n",
    "                nodes_per_block = int(1),\n",
    "                cores_per_node = int(16),   # Corresponds to --cpus-per-task                                                                                                                         \n",
    "                min_blocks = int(1),\n",
    "                max_blocks = int(10),\n",
    "                parallelism = 1,           # Was 0.80, 1 is \"use everything you can NOW\"                                                                                                            \n",
    "                exclusive = False,         # Default is T, hard to get workers on shared cluster                                                                                                    \n",
    "                walltime='12:00:00',       # Will limit job to this run time, 10 min default Parsl                                                                                                  \n",
    "                launcher=SrunLauncher()    # defaults to SingleNodeLauncher() which seems to work\n",
    "            ),\n",
    "            storage_access=[NoOpFileStaging()]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "parsl.load(config)\n",
    "print(\"Parsl configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Parsl workflow apps\n",
    "These apps are decorated with Parsl's `@bash_app` and as such are executed in parallel on the compute resources that are defined in the PW configuration loaded above. You can view the contents of the `@bash_app` as the same thing as the contents of an `sbatch` script. The difference between using Parsl and SLURM is that if you are using a SLURM cluster, Parsl will build your `sbatch` script for you and submit the job(s).  This job submission happens during Step 4, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining Parsl workflow apps...\")\n",
    "\n",
    "#===================================\n",
    "# Molecular dynamics simulation app\n",
    "#===================================\n",
    "@bash_app\n",
    "def run_alphafold(stdout='run.af.stdout', stderr='run.af.stderr', inputs=[], outputs=[]):\n",
    "    return '''\n",
    "    python run_singularity_container.py \\\n",
    "     --data_dir=/public/apps/alphafold/databases \\\n",
    "     --fasta_paths=/gs/gsfs0/users/gstefan/work/alphafold/input/%s \\\n",
    "     --max_template_date=2022-07-22 \\\n",
    "     --output_dir=/gs/gsfs0/users/gstefan/work/alphafold/output\n",
    "    ''' % (inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Workflow\n",
    "This cell executes the workflow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "print(\"Running Alphafold workflow...\")\n",
    "#============================================================================\n",
    "# For each line in cases.list, run and visualize a molecular dynamics simulation\n",
    "# These empty lists will store the futures of Parsl-parallelized apps.\n",
    "# Use Path for staging because multiple files in ./models/mdlite are needed\n",
    "# and mutliple files in ./results/case_*/md are sent back to the platform.\n",
    "futures = []\n",
    "for ii, fasta in enumerate(fasta_list):        \n",
    "    # Run simulation\n",
    "    futures.append(run_af(inputs=[workflow_dir+fasta]))\n",
    "    \n",
    "# Call results for all app futures to require\n",
    "# execution to wait for all simulations to complete.\n",
    "for instance in futures:\n",
    "    instance.result()\n",
    "\n",
    "print('All proteins done running.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: View results\n",
    "3D interactive visualization of proteins will be added later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Clean up\n",
    "This step is only necessary when running directly in a notebook. These intermediate and log files are removed to keep the workflow file structure clean if this workflow is pushed into the PW Market Place.  Please feel free to comment out these lines in order to inspect intermediate files as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 04:16:10 parsl.executors.status_handling:111 [ERROR]  Setting bad state due to exception\n",
      "Exception: 1. Failed to start block 0: Executor slurm failed due to: Attempts to provision nodes via provider has failed\n",
      "\n",
      "2022-08-13 04:16:10 parsl.dataflow.dflow:1057 [INFO]  DFK cleanup initiated\n",
      "2022-08-13 04:16:10 parsl.dataflow.dflow:947 [INFO]  Summary of tasks in DFK:\n",
      "2022-08-13 04:16:10 parsl.dataflow.dflow:967 [INFO]  End of summary\n",
      "2022-08-13 04:16:10 parsl.dataflow.dflow:1081 [INFO]  Terminating flow_control and strategy threads\n",
      "2022-08-13 04:16:10 parsl.dataflow.dflow:1111 [INFO]  DFK cleanup complete\n"
     ]
    }
   ],
   "source": [
    "if (run_in_notebook):\n",
    "    # Delete intermediate files/logs that are NOT core code or results\n",
    "    !rm -rf runinfo\n",
    "    !rm -rf __pycache__\n",
    "    !rm -rf parsl-task.*\n",
    "    !rm -rf *.pid\n",
    "    !rm -rf *.started\n",
    "    !rm -rf *.ended\n",
    "    !rm -rf *.cancelled\n",
    "    !rm -rf *.cogout\n",
    "    !rm -rf lastid*\n",
    "    !rm -rf launchcmd.*\n",
    "    !rm -rf parsl-htex-worker.sh\n",
    "    # Retain pw.conf if re-running this notebook on the \n",
    "    # same resource and there is no resource Off/On cycling.\n",
    "    # (See README.md for more information.)\n",
    "    !rm -rf pw.conf\n",
    "    # Delete outputs\n",
    "    #!rm -rf ./results\n",
    "    #!rm -f mdlite_dex.*\n",
    "    \n",
    "    # shut down the parsl executor\n",
    "    parsl.dfk().cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:parsl-1.2]",
   "language": "python",
   "name": "conda-env-parsl-1.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
